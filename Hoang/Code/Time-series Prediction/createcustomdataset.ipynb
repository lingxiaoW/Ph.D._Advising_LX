{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bpjUnRr3XPUK5H8vgGMTl1w7OkHPp8nf",
      "authorship_tag": "ABX9TyO6swi7jZnr3Dv5v8ARhJ/G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lingxiaoW/Ph.D._Advising_LX/blob/main/Hoang/Code/Time-series%20Prediction/createcustomdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4mPL3z9ZPYF",
        "outputId": "bb20f3c9-1cdb-41cf-e3fd-6c92a2d98bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the dataset **"
      ],
      "metadata": {
        "id": "3ye2-jgem_1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "568gJOtDh_8d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "#Define the dataset\n",
        "class TimeSeriesDataset(Dataset):\n",
        "  def __init__(self, file_paths, window_size, feature_col=\"temperature\"):\n",
        "    self.file_paths = file_paths\n",
        "    self.window_size = window_size\n",
        "    self.feature_col = feature_col\n",
        "    self.data, self.mean, self.std = self._load_data_from_files(file_paths, window_size, feature_col)\n",
        "\n",
        "  def _load_data_from_files(self, file_paths, window_size, feature_col='temperature'):\n",
        "    \"\"\"\n",
        "    Load time series data from CSV files, normalize it, and create backward sequences.\n",
        "\n",
        "    Parameters:\n",
        "    - file_paths (list): List of paths to CSV files.\n",
        "    - window_size (int): Number of timesteps in each input sequence.\n",
        "    - feature_col (str): Name of the column in the CSV to use as the time series feature.\n",
        "\n",
        "    Returns:\n",
        "    - data (list): List of tuples containing (input_sequence, target_value).\n",
        "    - mean (float): Mean used for normalization.\n",
        "    - std (float): Standard deviation used for normalization.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    mean = None\n",
        "    std = None\n",
        "\n",
        "    for file_path in file_paths:\n",
        "      #Load CSV file\n",
        "      try:\n",
        "        df = pd.read_csv(file_path, parse_dates=['date'])\n",
        "        print('Data loaded successfully!')\n",
        "        print(df.head())\n",
        "        print('Columns in dataset:', df.columns.tolist())\n",
        "      except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        continue # Skip to the next file if not found\n",
        "      except Exception as e:\n",
        "          print(f\"File not found: {file_path}\")\n",
        "          continue # Skip to the next file if not found\n",
        "\n",
        "      #Extract the time series ata\n",
        "      if feature_col not in df.columns:\n",
        "          print(f\"Feature column '{feature_col}' not found in the dataset in file {file_path}.\")\n",
        "          continue # Skip to the next file if feature column is not found\n",
        "      time_series = df[feature_col].values.astype(np.float32)\n",
        "\n",
        "      # Normalize the data\n",
        "      mean = np.mean(time_series)\n",
        "      std = np.std(time_series)\n",
        "      if std == 0:\n",
        "          print('Standard deviation is zero. Cannot normalize the data for file {file_path}.')\n",
        "          continue # Skip to the next file if std is zero\n",
        "      time_series = (time_series - mean) / std\n",
        "      print(f'Normalized with mean={mean:.2f}, std={std:.2f} for file {file_path}')\n",
        "\n",
        "      #Create time series sequences\n",
        "      for i in range(window_size, len(time_series)):\n",
        "        input_seq = time_series[i-(window_size-1): i+1]\n",
        "        target_value = time_series[i]\n",
        "        data.append((input_seq, target_value))\n",
        "    return data, mean, std\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    input_seq, target_value = self.data[idx]\n",
        "    return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_value, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "file_path = ['/content/daily_temperature.csv']\n",
        "window_size = 4\n",
        "feature_col = 'temperature'\n",
        "\n",
        "#create PyTorch dataset\n",
        "dataset = TimeSeriesDataset(file_path, window_size, feature_col)\n",
        "\n",
        "#Split dataset for training and testing\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "#create DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "lVZpkU7RbAWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe1ee2b-2256-4732-b31d-a3277c54ee3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "        date  temperature\n",
            "0 2021-01-01         30.5\n",
            "1 2021-01-02         31.0\n",
            "2 2021-01-03         32.1\n",
            "3 2021-01-04         31.8\n",
            "4 2021-01-05         33.0\n",
            "Columns in dataset: ['date', 'temperature']\n",
            "Normalized with mean=35.58, std=2.80 for file /content/daily_temperature.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-GB3mgjbxka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}