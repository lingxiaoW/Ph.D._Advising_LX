**09-08-2025**
* **Deep Learning-based Satellite Data Retrieval**
  * [x] Check the proposal (Project Narrative - New) in the folder "D:\Github\Ph.D._Advising_LX\Hoang\Reference\Deep Learning-based Satellite Data Retrieval"
  * [x] Get familiar with two objectives presented in the proposal
    * Objective 1: DL Model Design

    **9/8**: Read Objective 1 - DL Model Design overview: Focus on developing novel deep learning architectures to supplant computationally intensive components in traditional data retrieval algorithms.
    **Key insights**: The project's core innovation lies in leveraging DL to accelerate data retrieval by approximating expensive simulations (e.g., VRT calculations) with learned models. The time-series nature of the data necessitates architectures that handle sequential patterns, explaining the choice of CNN (for local feature extraction) and RNN (for long-range dependencies).
        
    **9/9**: Cont. reading Objective 1 - Sliding Window Technique: To exploit the time-series structure, a sliding window approach transforms the data into sequential inputs suitable for DL models.
    **Input/Output**: Input = 14 features, output = 22 features (VRT-generated)
    **The sliding window** (window_size) is a tuning parameter, based on the prediction results on testing dataset (How can we decide the window size for the first time???)
    
    **9/10**: Cont. reading Objective 1 - Progress on CNN Model Design:

    **CNN Architecture**: Proposal: Multiple convolutional + max pooling layers, flatten, then fully connected to output.
    * Convolutional Layers: 1D filters
    * Max-pooling: Downsample by taking max in pools (size 2-3), reducing dimensionality (e.g., from w to w/2) and preventing overfitting.
    * Flatten Layer: Converts pooled maps to 1D vector.
    * Fully Connected Layers: Dense layers (e.g., 128-256 nodes) with ReLU, mapping to 22-output vector. Final activation: Linear for regression.

    **9/11 - RNN Architecture**: Proposal: uses LSTM cells for temporal dependencies. Layers: input sequences -> Multiple LSTMs (with dropout) -> Fully connected
  -> Output y.

    **Why RNN (LSTM)?** Data: sequential dependencies. LSTMs handle long-term memory making them ideal for variable-length windows.
  
    * Objective 2: DL Model Training
   
    **9/11** Two approaches: Supervised (uses labeled x-y pairs) and Reinforcement Learning (trial-and-error without full lables).
      **Key points** 
      * Supervise Learning: train to minimize MSE. Uses pre-labled data (VRT - generated y). Updates weights via backprop to map x -> y.
      * Deep Reinforcement Learning: no direct y lables needed. Windowed x -> DL - vector y -> Plug into X^2(x) -> Update DL to max reward.
  * [ ] Prepare a CNN and a RNN model for Objective 1



* **Set up the lab computer**
  * [ ] Schedule an appointment with the IT department to install remote access on your desktop to remotely access the DL training desktop.  
  **9/9**
    * Installed VS code on desktop
    * Requested IT to install remote access to the DL training desktop

     **9/10**
     * Call IT support to remind the remote access ticket
