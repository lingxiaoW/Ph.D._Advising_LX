### Facial Expression Recognition
* Start with a training dataset. 
    * FER2013 ([link](https://www.kaggle.com/datasets/msambare/fer2013))
    * DIPSER ([link](https://www.scidb.cn/en/detail?dataSetId=7856c716c0cc4589a23ee4a23d8a0893))
* You can use a pre-trained model (CNN from stratch).
    * RESNet 50 as the backbone
    * [ ] The accuracy is not high
    * [ ] **Improve the performance**. (> 0.7)
    * [ ] Show the testing image in a row. (you can use the image from the testing dataset).



* **GPT-based Method**
    * We are more interested in providing an image of classroom to the GPT and ask it to tell which students are engaged, which students are not engaged. 
        * [x] Detect humans and human faces from the classroom images.
        * [ ] Process the training data to locate the label of time when the image was taken.
        * [x] Crop the humans and human faces from the classroom images.
        * [ ] Do the **body-based** engage or not engage with LLM
        * [ ] Do the **facial expression** classification with LLM (try LLM first and then CNN),
            * [ ] Output should be in scalar between 1 to 5 and this scalar indicates the attention level 
            * [ ] Pick 30 examples engaged (10 pictures: attention score: 4 - 5), neutral , 10 pictures: 3, disengaged, 10 pictures: 1 - 2; Use attention score from self labeling 
            * [ ] Summarize sevaral rules based on 30 examples, make 3 diagrams: one for yaw, one for pitch, and one for roll: Figure out the numbers for engaged, not engaged and neutral
            * [ ] emotion candidates should be 9 to cover all possible labels in the training dataset
    
    * The output image should be bounding boxes of students with labels like engaged/not engaged. 