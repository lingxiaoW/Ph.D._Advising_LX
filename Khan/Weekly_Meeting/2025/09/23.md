## 09-23
* The final product is similar to Table 1 in DriveGPT4 (driveLME).
    * We provide front view camera images and LLM gives the action
    * Our final goal is to implement this on a robotic vehicle in a simulated traffic environment. 
    * Find a few open source projects to get familiar with this concept and then think about to how modify it. 

* [ ] Literature review: Find articles of LLM-based autonomous driving applications. 


* Try these:
    * [ ] how to control the vehicle using the code
        * Explain here using pesduo code

        * Behavior Agent (route setup): 
            - 
            - agent.set_destination (destination)
            - route = global_route_planner.trace_route(start=ego.get_location(), destination)
            - output: route: List[(Waypoint, RoadOption)]
            - local_planner.set_global_plan(route, stop_waypoint_creation=True, clean_queue=True)
            - Output: waypoints queue in LocalPlanner
        * Behavior Agent (per frame):
            - 
            - if traffic_light_manager / collisions / pedestrians
            - emergency_stop() 
            - Output: VehicleControl(throttle=0, brake=max, steer=keep)
            - local_planner.set_speed(v_target)  (from TTC/junction logic)
            - local_planner.run_step()
            - Output: VehicleControl from PID (throttle/brake/steer)
            - apply_control(VehicleControl) -- Update vehicle at every frame 

        * Local Planner (per frame):
            - 
            - target_waypoint = queue.front()
            - pid.run_step(target_speed, target_waypoint)
            - final VehicleControl (throttle/steer/brake)

        * PID Controller (called by Local Planner):
            - 
            - accel  = lon_pid.run_step(target_speed)
            - Output: throttle/brake
            - steer  = lat_pid.run_step(target_waypoint)
            - Output: steer
            - VehicleControl(throttle, brake, steer)

        * **output** = LLM(input)
            * Is the **output** executed or not?
                - No. Couldn'tfind the gpt model path. 
                - Correct way:

                    from openai import OpenAI
                    
                    client = OpenAI()  # picks up OPENAI_API_KEY

        * If the LLM gives "FOLLOW", how Carla intreprate this command to control the vehicle?
            - apply the function apply_target_speed -> convert the FOLLOW| SLOW| STOP| GO into target speed (numerical).
    * [ ] check the LLM reasoning result. 
        * Use the debug function to see the LLM input and output at each time step. 
        * I need to see inputs and outputs to the LLM and check whether the vehcile is following the LLM commands. 
        * It is not showing me a video becuase you don't know what is right what is wrong in the video. 
