### Facial Expression Recognition
* Start with a training dataset. 
    * FER2013 ([link](https://www.kaggle.com/datasets/msambare/fer2013))
    * DIPSER ([link](https://www.scidb.cn/en/detail?dataSetId=7856c716c0cc4589a23ee4a23d8a0893))
* You can use a pre-trained model (CNN from stratch).
    * RESNet 50 as the backbone
    * [ ] The accuracy is not high
    * [ ] **Improve the performance**. (> 0.7)
    * [ ] Show the testing image in a row. (you can use the image from the testing dataset).

<br />

* **Paper Review**
    * [ ] **06-19**: send me the word document and add a section "acknowledgement" mentioning diego and Dr. peters. 
     

<br />

* **GPT-based Method**
    * We are more interested in providing an image of classroom to the GPT and ask it to tell which students are engaged, which students are not engaged. 
        * [ ] Do the **body-based** engage or not engage with LLM
        * [ ] Do the **facial expression** classification with LLM (try LLM first and then CNN),
            * [ ] **Head position hint**: Pick 30 examples engaged (10 pictures: attention score: 4 - 5), neutral , 10 pictures: 3, disengaged, 10 pictures: 1 - 2; Use attention score from self labeling
            * [ ] **Facial Expression hint**: 
                * [ ] Provide examples in the prompt (like dilu). start with 3 -> 2 -> 1.
                * [ ] How to determine the attention level based on facial and head position? 
                    * Determining attention scores purely based on yaw angle is not OK.
                    * [ ] **06-09**: just use images of self-labeling. 
                        * **06-19**: Shrink the emotion options to **3**. Use this [tutorial](https://imotions.com/blog/learning/research-fundamentals/facial-action-coding-system/) to analyze the emtion. 
                        * one emtion pick one example. 
                        * use the remaining images as the testing. Don't randomly throw an image to test.
                    * [ ] **06-25**: finetune the model to improve the performance. 
                        * provide 4 examples in negative emotion.
                        * add eye direction to calculate attention score. 
                        * find a US conference/journal (related to education, or AI). Must be IEEE or ACM. (make a list)
                        * **07-03**: write the experiment part first. 
                            * Subsection 1: Some sample trials: provide several examples and corresponding LLM response. One example for one emotion.
                            * Subsection 2: Statistic analysis. randomly choose 20 or 30 images (from self-labeling). Compare the attention score. More important is the attention score. Compare it with CNN prediction.
                    

                    * Perhaps you can associate the emtion with attention scores

                        2. Determine the emtion first using facial actions
                        3. Positive emtions (hope, enjoyment, pride) are asscoated with high attention scores.
                        4. Use head posture to fine determining the attention score. 
                        5. Just focus on images with self labeling (print both predicted/ground truth emtions & attention score.) 
                        
    
    * The output image should be bounding boxes of students with labels like engaged/not engaged. 